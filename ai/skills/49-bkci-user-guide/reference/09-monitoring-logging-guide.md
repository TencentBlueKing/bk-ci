# ç›‘æ§ä¸æ—¥å¿—ç®¡ç†æŒ‡å—

## æ¦‚è¿°

è“ç›¾æä¾›äº†å…¨é¢çš„ç›‘æ§å’Œæ—¥å¿—ç®¡ç†èƒ½åŠ›ï¼Œå¸®åŠ©ç”¨æˆ·å®æ—¶äº†è§£æµæ°´çº¿æ‰§è¡ŒçŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œé—®é¢˜æ’æŸ¥ã€‚é€šè¿‡åˆç†ä½¿ç”¨ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½ï¼Œå¯ä»¥æé«˜ç³»ç»Ÿå¯è§‚æµ‹æ€§ï¼Œå¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚

## æ„å»ºæ—¥å¿—ç®¡ç†

### 1. æ—¥å¿—æŸ¥çœ‹

#### å®æ—¶æ—¥å¿—æŸ¥çœ‹
```yaml
# åœ¨æµæ°´çº¿æ‰§è¡Œè¿‡ç¨‹ä¸­æŸ¥çœ‹å®æ—¶æ—¥å¿—
- name: "æŸ¥çœ‹å®æ—¶æ—¥å¿—"
  run: |
    echo "å¼€å§‹æ‰§è¡Œæ„å»ºä»»åŠ¡"
    
    # è¾“å‡ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—
    echo "$(date '+%Y-%m-%d %H:%M:%S') - å¼€å§‹ç¼–è¯‘"
    
    # è¾“å‡ºå½©è‰²æ—¥å¿—ï¼ˆæ”¯æŒANSIé¢œè‰²ç ï¼‰
    echo -e "\033[32mæˆåŠŸ: ç¼–è¯‘å®Œæˆ\033[0m"
    echo -e "\033[31mé”™è¯¯: å‘ç°é—®é¢˜\033[0m"
    echo -e "\033[33mè­¦å‘Š: éœ€è¦æ³¨æ„\033[0m"
    
    # è¾“å‡ºç»“æ„åŒ–æ—¥å¿—
    echo "##[section]å¼€å§‹æµ‹è¯•é˜¶æ®µ"
    echo "##[debug]è°ƒè¯•ä¿¡æ¯: å˜é‡å€¼ä¸º $VAR_NAME"
    echo "##[warning]è­¦å‘Š: é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°"
    echo "##[error]é”™è¯¯: è¿æ¥æ•°æ®åº“å¤±è´¥"
```

#### æ—¥å¿—çº§åˆ«æ§åˆ¶
```yaml
# è®¾ç½®æ—¥å¿—çº§åˆ«
- name: "é…ç½®æ—¥å¿—çº§åˆ«"
  env:
    LOG_LEVEL: "DEBUG"  # DEBUG, INFO, WARN, ERROR
  run: |
    case $LOG_LEVEL in
      "DEBUG")
        echo "##[debug]è°ƒè¯•æ¨¡å¼å·²å¯ç”¨"
        set -x  # æ˜¾ç¤ºæ‰§è¡Œçš„å‘½ä»¤
        ;;
      "INFO")
        echo "##[info]ä¿¡æ¯æ¨¡å¼"
        ;;
      "WARN")
        echo "##[warning]è­¦å‘Šæ¨¡å¼"
        ;;
      "ERROR")
        echo "##[error]é”™è¯¯æ¨¡å¼"
        ;;
    esac
```

### 2. æ—¥å¿—ä¸‹è½½å’Œå¯¼å‡º

#### ä¸‹è½½æ’ä»¶æ‰§è¡Œæ—¥å¿—
1. åœ¨æ’ä»¶æ‰§è¡Œç•Œé¢å³ä¸Šè§’ç‚¹å‡»ä¸‰ç‚¹å›¾æ ‡
2. é€‰æ‹©"ä¸‹è½½æ—¥å¿—"æŒ‰é’®
3. æ—¥å¿—å°†ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ä¾›åç»­åˆ†æ

#### æ‰¹é‡æ—¥å¿—å¯¼å‡º
```bash
# ä½¿ç”¨APIæ‰¹é‡ä¸‹è½½æ„å»ºæ—¥å¿—
#!/bin/bash

PROJECT_ID="your-project-id"
PIPELINE_ID="your-pipeline-id"
BUILD_ID="your-build-id"
TOKEN="your-access-token"

# è·å–æ„å»ºè¯¦æƒ…
curl -H "Authorization: Bearer $TOKEN" \
  "https://devops.woa.com/ms/process/api/user/projects/$PROJECT_ID/pipelines/$PIPELINE_ID/builds/$BUILD_ID/detail" \
  -o build_detail.json

# ä¸‹è½½å®Œæ•´æ„å»ºæ—¥å¿—
curl -H "Authorization: Bearer $TOKEN" \
  "https://devops.woa.com/ms/log/api/user/projects/$PROJECT_ID/pipelines/$PIPELINE_ID/builds/$BUILD_ID/logs/download" \
  -o "build_${BUILD_ID}_logs.txt"

# ä¸‹è½½ç‰¹å®šæ’ä»¶æ—¥å¿—
ELEMENT_ID="your-element-id"
curl -H "Authorization: Bearer $TOKEN" \
  "https://devops.woa.com/ms/log/api/user/projects/$PROJECT_ID/pipelines/$PIPELINE_ID/builds/$BUILD_ID/logs/$ELEMENT_ID/download" \
  -o "element_${ELEMENT_ID}_logs.txt"
```

### 3. æ—¥å¿—åˆ†æå’Œæœç´¢

#### æ—¥å¿—æœç´¢è¯­æ³•
```bash
# åŸºæœ¬æœç´¢
grep "ERROR" build.log

# æ—¶é—´èŒƒå›´æœç´¢
grep "2024-01-15 10:" build.log

# æ­£åˆ™è¡¨è¾¾å¼æœç´¢
grep -E "ERROR|FATAL|Exception" build.log

# å¤šæ–‡ä»¶æœç´¢
grep -r "æ„å»ºå¤±è´¥" logs/

# ç»Ÿè®¡é”™è¯¯æ•°é‡
grep -c "ERROR" build.log

# æ˜¾ç¤ºé”™è¯¯å‰åä¸Šä¸‹æ–‡
grep -A 5 -B 5 "ERROR" build.log
```

#### ç»“æ„åŒ–æ—¥å¿—åˆ†æ
```python
#!/usr/bin/env python3
import json
import re
from datetime import datetime

def analyze_build_log(log_file):
    """åˆ†ææ„å»ºæ—¥å¿—ï¼Œæå–å…³é”®ä¿¡æ¯"""
    
    stats = {
        'total_lines': 0,
        'errors': [],
        'warnings': [],
        'duration': {},
        'plugins': {}
    }
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            stats['total_lines'] += 1
            
            # æå–é”™è¯¯ä¿¡æ¯
            if 'ERROR' in line or 'FATAL' in line:
                stats['errors'].append({
                    'line': stats['total_lines'],
                    'message': line.strip(),
                    'timestamp': extract_timestamp(line)
                })
            
            # æå–è­¦å‘Šä¿¡æ¯
            if 'WARNING' in line or 'WARN' in line:
                stats['warnings'].append({
                    'line': stats['total_lines'],
                    'message': line.strip(),
                    'timestamp': extract_timestamp(line)
                })
            
            # æå–æ’ä»¶æ‰§è¡Œæ—¶é—´
            plugin_match = re.search(r'Plugin (\w+) completed in (\d+)ms', line)
            if plugin_match:
                plugin_name = plugin_match.group(1)
                duration = int(plugin_match.group(2))
                stats['plugins'][plugin_name] = duration
    
    return stats

def extract_timestamp(line):
    """ä»æ—¥å¿—è¡Œä¸­æå–æ—¶é—´æˆ³"""
    timestamp_pattern = r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'
    match = re.search(timestamp_pattern, line)
    return match.group(0) if match else None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    stats = analyze_build_log('build.log')
    
    print(f"æ€»è¡Œæ•°: {stats['total_lines']}")
    print(f"é”™è¯¯æ•°: {len(stats['errors'])}")
    print(f"è­¦å‘Šæ•°: {len(stats['warnings'])}")
    
    if stats['plugins']:
        print("\næ’ä»¶æ‰§è¡Œæ—¶é—´:")
        for plugin, duration in stats['plugins'].items():
            print(f"  {plugin}: {duration}ms")
```

## æµæ°´çº¿ç›‘æ§

### 1. ç›‘æ§æŒ‡æ ‡é…ç½®

#### åŸºç¡€ç›‘æ§æŒ‡æ ‡
è“ç›¾æä¾›ä»¥ä¸‹æ ¸å¿ƒç›‘æ§æŒ‡æ ‡ï¼š

```yaml
# æµæ°´çº¿çŠ¶æ€ç›‘æ§
pipeline_status_info:
  labels:
    - pipelineId: "æµæ°´çº¿ID"
    - buildId: "æ„å»ºID" 
    - status: "SUCCEED|FAILED|CANCELED"
    - projectId: "é¡¹ç›®ID"
    - triggerUser: "è§¦å‘ç”¨æˆ·"
    - pipelineName: "æµæ°´çº¿åç§°"
    - trigger: "TIME_TRIGGER|MANUAL|WEB_HOOK|REMOTE"
    - eventType: "BUILD_END"

# æµæ°´çº¿è¿è¡Œæ—¶é—´ç›‘æ§
pipeline_running_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - projectId: "é¡¹ç›®ID"
  value: "è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"

# æµæ°´çº¿æ’é˜Ÿæ—¶é—´ç›‘æ§
pipeline_queue_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - projectId: "é¡¹ç›®ID"
  value: "æ’é˜Ÿæ—¶é—´ï¼ˆç§’ï¼‰"
```

#### æ’ä»¶çº§åˆ«ç›‘æ§
```yaml
# æ’ä»¶çŠ¶æ€ç›‘æ§
pipeline_step_status_info:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - step_id: "æ­¥éª¤ID"
    - status: "SUCCEED|FAILED|RUNNING"
    - projectId: "é¡¹ç›®ID"

# æ’ä»¶è¿è¡Œæ—¶é—´ç›‘æ§
pipeline_step_running_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - step_id: "æ­¥éª¤ID"
    - job_id: "Job ID"
    - projectId: "é¡¹ç›®ID"
  value: "è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"
```

#### Jobçº§åˆ«ç›‘æ§
```yaml
# Jobè¿è¡Œæ—¶é—´ç›‘æ§
pipeline_job_running_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - job_id: "Job ID"
    - projectId: "é¡¹ç›®ID"
  value: "è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"

# Jobæ’é˜Ÿæ—¶é—´ç›‘æ§
pipeline_job_queue_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - projectId: "é¡¹ç›®ID"
    - mutexGroup: "äº’æ–¥ç»„åç§°"
    - agentReuseMutex: "æ„å»ºæœºäº’æ–¥ID"
  value: "æ’é˜Ÿæ—¶é—´ï¼ˆç§’ï¼‰"
```

#### æ„å»ºæœºç›‘æ§
```yaml
# æ„å»ºæœºè¿è¡Œæ—¶é—´ç›‘æ§
pipeline_agent_running_time_seconds:
  labels:
    - pipeline_id: "æµæ°´çº¿ID"
    - build_id: "æ„å»ºID"
    - projectId: "é¡¹ç›®ID"
    - agentIp: "æ„å»ºæœºIP"
    - agentId: "æ„å»ºæœºID"
    - nodeHashId: "èŠ‚ç‚¹ID"
    - envHashId: "ç¯å¢ƒID"
  value: "è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"
```

### 2. ç›‘æ§ä»ªè¡¨ç›˜é…ç½®

#### åˆ›å»ºç›‘æ§ä»ªè¡¨ç›˜
1. è®¿é—®[è“é²¸ç›‘æ§å¹³å°](https://bkm.woa.com/)
2. é€‰æ‹©å¯¹åº”çš„ç ”å‘é¡¹ç›®
3. åˆ›å»ºæ–°çš„ä»ªè¡¨ç›˜
4. æ·»åŠ å¯è§†åŒ–å›¾è¡¨

#### æµæ°´çº¿æˆåŠŸç‡ç›‘æ§
```yaml
# ä»ªè¡¨ç›˜é…ç½®ç¤ºä¾‹
dashboard_config:
  title: "æµæ°´çº¿ç›‘æ§ä»ªè¡¨ç›˜"
  panels:
    - title: "æµæ°´çº¿æˆåŠŸç‡"
      type: "stat"
      targets:
        - expr: |
            (
              sum(rate(pipeline_status_info{status="SUCCEED",pipelineId="$pipeline_id"}[5m])) /
              sum(rate(pipeline_status_info{eventType="BUILD_END",pipelineId="$pipeline_id"}[5m]))
            ) * 100
      options:
        unit: "percent"
        min: 0
        max: 100
        thresholds:
          - color: "red"
            value: 80
          - color: "yellow" 
            value: 90
          - color: "green"
            value: 95
```

#### æµæ°´çº¿æ‰§è¡Œæ—¶é—´è¶‹åŠ¿
```yaml
- title: "æµæ°´çº¿æ‰§è¡Œæ—¶é—´è¶‹åŠ¿"
  type: "graph"
  targets:
    - expr: |
        max by (pipeline_id, projectId) (
          max_over_time(
            pipeline_running_time_seconds{
              pipeline_id="$pipeline_id",
              projectId="$project_id"
            }[1m]
          )
        )
  options:
    legend:
      show: true
    tooltip:
      shared: true
    yAxis:
      unit: "seconds"
      label: "æ‰§è¡Œæ—¶é—´"
```

#### æ„å»ºæœºå¹¶å‘ç›‘æ§
```yaml
- title: "æ„å»ºæœºå¹¶å‘æƒ…å†µ"
  type: "graph"
  targets:
    - expr: |
        count by (agentIp) (
          pipeline_agent_running_time_seconds{
            projectId="$project_id"
          }
        )
  options:
    legend:
      show: true
      values: true
    yAxis:
      label: "å¹¶å‘æ•°"
      min: 0
```

### 3. å‘Šè­¦é…ç½®

#### æµæ°´çº¿å¤±è´¥å‘Šè­¦
```yaml
alert_rules:
  - name: "æµæ°´çº¿æ‰§è¡Œå¤±è´¥å‘Šè­¦"
    expr: |
      increase(pipeline_status_info{
        status="FAILED",
        pipelineId="$pipeline_id"
      }[5m]) > 0
    for: "0m"
    labels:
      severity: "warning"
      team: "dev-team"
    annotations:
      summary: "æµæ°´çº¿æ‰§è¡Œå¤±è´¥"
      description: |
        æµæ°´çº¿ {{ $labels.pipelineName }} æ‰§è¡Œå¤±è´¥
        é¡¹ç›®: {{ $labels.projectId }}
        æ„å»ºID: {{ $labels.buildId }}
        è§¦å‘ç”¨æˆ·: {{ $labels.triggerUser }}
        
        [æŸ¥çœ‹è¯¦æƒ…](https://devops.woa.com/console/pipeline/{{ $labels.projectId }}/{{ $labels.pipelineId }}/detail/{{ $labels.buildId }}/executeDetail)
```

#### æ‰§è¡Œæ—¶é—´è¶…æ—¶å‘Šè­¦
```yaml
- name: "æµæ°´çº¿æ‰§è¡Œè¶…æ—¶å‘Šè­¦"
  expr: |
    pipeline_running_time_seconds{
      pipeline_id="$pipeline_id"
    } > 1800  # 30åˆ†é’Ÿ
  for: "1m"
  labels:
    severity: "warning"
  annotations:
    summary: "æµæ°´çº¿æ‰§è¡Œæ—¶é—´è¿‡é•¿"
    description: |
      æµæ°´çº¿æ‰§è¡Œæ—¶é—´è¶…è¿‡30åˆ†é’Ÿ
      å½“å‰æ‰§è¡Œæ—¶é—´: {{ $value }}ç§’
      
      [æŸ¥çœ‹è¯¦æƒ…](https://devops.woa.com/console/pipeline/{{ $labels.projectId }}/{{ $labels.pipeline_id }}/detail/{{ $labels.build_id }}/executeDetail)
```

#### æ’ä»¶æ‰§è¡Œè¶…æ—¶å‘Šè­¦
```yaml
- name: "æ’ä»¶æ‰§è¡Œè¶…æ—¶å‘Šè­¦"
  expr: |
    pipeline_step_running_time_seconds{
      step_id="$step_id",
      pipeline_id="$pipeline_id"
    } > 600  # 10åˆ†é’Ÿ
  for: "30s"
  labels:
    severity: "critical"
  annotations:
    summary: "æ’ä»¶æ‰§è¡Œè¶…æ—¶"
    description: |
      æ’ä»¶ {{ $labels.step_id }} æ‰§è¡Œæ—¶é—´è¶…è¿‡10åˆ†é’Ÿ
      å½“å‰æ‰§è¡Œæ—¶é—´: {{ $value }}ç§’
      æµæ°´çº¿: {{ $labels.pipeline_id }}
      æ„å»º: {{ $labels.build_id }}
```

#### æ„å»ºæœºèµ„æºå‘Šè­¦
```yaml
- name: "æ„å»ºæœºå¹¶å‘è¿‡é«˜å‘Šè­¦"
  expr: |
    count by (agentIp) (
      pipeline_agent_running_time_seconds{
        projectId="$project_id"
      }
    ) > 5  # å•ä¸ªæ„å»ºæœºå¹¶å‘è¶…è¿‡5ä¸ª
  for: "2m"
  labels:
    severity: "warning"
  annotations:
    summary: "æ„å»ºæœºå¹¶å‘è¿‡é«˜"
    description: |
      æ„å»ºæœº {{ $labels.agentIp }} å½“å‰å¹¶å‘æ•°: {{ $value }}
      å»ºè®®æ£€æŸ¥æ„å»ºæœºè´Ÿè½½æƒ…å†µ
```

## æ€§èƒ½ç›‘æ§

### 1. ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡

#### æ„å»ºæ€§èƒ½åˆ†æ
```python
#!/usr/bin/env python3
import requests
import json
from datetime import datetime, timedelta

class BuildPerformanceAnalyzer:
    def __init__(self, project_id, token):
        self.project_id = project_id
        self.token = token
        self.base_url = "https://devops.woa.com/ms"
        
    def get_build_metrics(self, pipeline_id, days=7):
        """è·å–æµæ°´çº¿æ€§èƒ½æŒ‡æ ‡"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        # è·å–æ„å»ºå†å²
        builds = self._get_build_history(pipeline_id, start_time, end_time)
        
        metrics = {
            'total_builds': len(builds),
            'success_rate': 0,
            'avg_duration': 0,
            'max_duration': 0,
            'min_duration': float('inf'),
            'failure_reasons': {},
            'performance_trend': []
        }
        
        successful_builds = 0
        total_duration = 0
        
        for build in builds:
            duration = build.get('totalTime', 0) / 1000  # è½¬æ¢ä¸ºç§’
            
            if build.get('status') == 'SUCCEED':
                successful_builds += 1
                total_duration += duration
                
                metrics['max_duration'] = max(metrics['max_duration'], duration)
                metrics['min_duration'] = min(metrics['min_duration'], duration)
            else:
                # ç»Ÿè®¡å¤±è´¥åŸå› 
                reason = build.get('errorInfo', {}).get('errorMsg', 'Unknown')
                metrics['failure_reasons'][reason] = metrics['failure_reasons'].get(reason, 0) + 1
            
            # è®°å½•æ€§èƒ½è¶‹åŠ¿
            metrics['performance_trend'].append({
                'build_id': build.get('id'),
                'duration': duration,
                'status': build.get('status'),
                'start_time': build.get('startTime')
            })
        
        if successful_builds > 0:
            metrics['success_rate'] = (successful_builds / len(builds)) * 100
            metrics['avg_duration'] = total_duration / successful_builds
        
        if metrics['min_duration'] == float('inf'):
            metrics['min_duration'] = 0
            
        return metrics
    
    def _get_build_history(self, pipeline_id, start_time, end_time):
        """è·å–æ„å»ºå†å²è®°å½•"""
        url = f"{self.base_url}/process/api/user/projects/{self.project_id}/pipelines/{pipeline_id}/builds"
        
        params = {
            'page': 1,
            'pageSize': 100,
            'startTimeStartTime': int(start_time.timestamp() * 1000),
            'startTimeEndTime': int(end_time.timestamp() * 1000)
        }
        
        headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        
        response = requests.get(url, params=params, headers=headers)
        if response.status_code == 200:
            data = response.json()
            return data.get('data', {}).get('records', [])
        else:
            return []
    
    def generate_report(self, pipeline_id):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        metrics = self.get_build_metrics(pipeline_id)
        
        report = f"""
# æµæ°´çº¿æ€§èƒ½æŠ¥å‘Š

## åŸºç¡€æŒ‡æ ‡
- æ€»æ„å»ºæ¬¡æ•°: {metrics['total_builds']}
- æˆåŠŸç‡: {metrics['success_rate']:.2f}%
- å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['avg_duration']:.2f}ç§’
- æœ€é•¿æ‰§è¡Œæ—¶é—´: {metrics['max_duration']:.2f}ç§’
- æœ€çŸ­æ‰§è¡Œæ—¶é—´: {metrics['min_duration']:.2f}ç§’

## å¤±è´¥åŸå› åˆ†æ
"""
        
        for reason, count in metrics['failure_reasons'].items():
            report += f"- {reason}: {count}æ¬¡\n"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = BuildPerformanceAnalyzer("your-project-id", "your-token")
report = analyzer.generate_report("your-pipeline-id")
print(report)
```

### 2. èµ„æºä½¿ç”¨ç›‘æ§

#### æ„å»ºæœºèµ„æºç›‘æ§
```yaml
# æ„å»ºæœºCPUä½¿ç”¨ç‡ç›‘æ§
- name: "æ„å»ºæœºCPUç›‘æ§"
  expr: |
    avg by (agentIp) (
      rate(node_cpu_seconds_total{
        mode!="idle",
        instance=~"$agent_ip:.*"
      }[5m])
    ) * 100
  alert:
    condition: "> 80"
    duration: "5m"
    message: "æ„å»ºæœºCPUä½¿ç”¨ç‡è¿‡é«˜"

# æ„å»ºæœºå†…å­˜ä½¿ç”¨ç‡ç›‘æ§
- name: "æ„å»ºæœºå†…å­˜ç›‘æ§"
  expr: |
    (1 - (
      node_memory_MemAvailable_bytes{instance=~"$agent_ip:.*"} /
      node_memory_MemTotal_bytes{instance=~"$agent_ip:.*"}
    )) * 100
  alert:
    condition: "> 85"
    duration: "3m"
    message: "æ„å»ºæœºå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"

# æ„å»ºæœºç£ç›˜ä½¿ç”¨ç‡ç›‘æ§
- name: "æ„å»ºæœºç£ç›˜ç›‘æ§"
  expr: |
    (1 - (
      node_filesystem_avail_bytes{
        instance=~"$agent_ip:.*",
        fstype!="tmpfs"
      } /
      node_filesystem_size_bytes{
        instance=~"$agent_ip:.*",
        fstype!="tmpfs"
      }
    )) * 100
  alert:
    condition: "> 90"
    duration: "1m"
    message: "æ„å»ºæœºç£ç›˜ç©ºé—´ä¸è¶³"
```

### 3. ç½‘ç»œå’Œä¾èµ–ç›‘æ§

#### å¤–éƒ¨ä¾èµ–ç›‘æ§
```yaml
# ä»£ç åº“è¿æ¥ç›‘æ§
- name: "ä»£ç åº“è¿æ¥ç›‘æ§"
  expr: |
    probe_success{
      job="git-repo-probe",
      instance=~"git.code.oa.com.*"
    }
  alert:
    condition: "== 0"
    duration: "1m"
    message: "ä»£ç åº“è¿æ¥å¤±è´¥"

# åˆ¶å“åº“è¿æ¥ç›‘æ§
- name: "åˆ¶å“åº“è¿æ¥ç›‘æ§"
  expr: |
    probe_success{
      job="artifact-repo-probe",
      instance=~"bkrepo.woa.com.*"
    }
  alert:
    condition: "== 0"
    duration: "2m"
    message: "åˆ¶å“åº“è¿æ¥å¤±è´¥"

# å¤–éƒ¨APIå“åº”æ—¶é—´ç›‘æ§
- name: "å¤–éƒ¨APIå“åº”æ—¶é—´"
  expr: |
    probe_duration_seconds{
      job="external-api-probe"
    }
  alert:
    condition: "> 5"
    duration: "2m"
    message: "å¤–éƒ¨APIå“åº”æ—¶é—´è¿‡é•¿"
```

## æ—¥å¿—èšåˆå’Œåˆ†æ

### 1. æ—¥å¿—æ”¶é›†é…ç½®

#### ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
```bash
#!/bin/bash

# æ—¥å¿—å‡½æ•°åº“
log_info() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [INFO] $1" | tee -a build.log
}

log_warn() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [WARN] $1" | tee -a build.log
}

log_error() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [ERROR] $1" | tee -a build.log
}

log_debug() {
    if [[ "$LOG_LEVEL" == "DEBUG" ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S') [DEBUG] $1" | tee -a build.log
    fi
}

# JSONæ ¼å¼æ—¥å¿—
log_json() {
    local level=$1
    local message=$2
    local extra=${3:-"{}"}
    
    jq -n \
        --arg timestamp "$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)" \
        --arg level "$level" \
        --arg message "$message" \
        --argjson extra "$extra" \
        '{
            timestamp: $timestamp,
            level: $level,
            message: $message,
            pipeline_id: env.PIPELINE_ID,
            build_id: env.BUILD_ID,
            job_id: env.JOB_ID,
            extra: $extra
        }' >> structured.log
}

# ä½¿ç”¨ç¤ºä¾‹
log_info "å¼€å§‹æ„å»ºæµç¨‹"
log_json "INFO" "å¼€å§‹ç¼–è¯‘" '{"component": "compiler", "language": "java"}'
```

### 2. æ—¥å¿—åˆ†æå·¥å…·

#### ELK Stacké›†æˆ
```yaml
# Logstashé…ç½®ç¤ºä¾‹
input {
  file {
    path => "/var/log/bkci/builds/*.log"
    start_position => "beginning"
    codec => "json"
  }
}

filter {
  if [level] {
    mutate {
      add_field => { "log_level" => "%{level}" }
    }
  }
  
  if [pipeline_id] {
    mutate {
      add_field => { "pipeline" => "%{pipeline_id}" }
    }
  }
  
  # è§£æé”™è¯¯å †æ ˆ
  if [level] == "ERROR" and [message] =~ /Exception/ {
    grok {
      match => { 
        "message" => "(?<exception_type>\w+Exception): (?<exception_message>.*)"
      }
    }
  }
  
  # æ·»åŠ æ—¶é—´æˆ³
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "bkci-logs-%{+YYYY.MM.dd}"
  }
}
```

#### æ—¥å¿—æœç´¢å’Œåˆ†æ
```bash
# ElasticsearchæŸ¥è¯¢ç¤ºä¾‹

# æŸ¥è¯¢ç‰¹å®šæµæ°´çº¿çš„é”™è¯¯æ—¥å¿—
curl -X GET "elasticsearch:9200/bkci-logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        {"term": {"pipeline_id": "your-pipeline-id"}},
        {"term": {"level": "ERROR"}},
        {"range": {"@timestamp": {"gte": "now-1d"}}}
      ]
    }
  },
  "sort": [{"@timestamp": {"order": "desc"}}],
  "size": 100
}'

# èšåˆåˆ†æé”™è¯¯ç±»å‹
curl -X GET "elasticsearch:9200/bkci-logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "size": 0,
  "aggs": {
    "error_types": {
      "terms": {
        "field": "exception_type.keyword",
        "size": 10
      }
    },
    "error_timeline": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "1h"
      },
      "aggs": {
        "error_count": {
          "filter": {"term": {"level": "ERROR"}}
        }
      }
    }
  }
}'
```

### 3. æ™ºèƒ½æ—¥å¿—åˆ†æ

#### å¼‚å¸¸æ£€æµ‹
```python
#!/usr/bin/env python3
import re
import json
from collections import defaultdict, Counter
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self):
        self.error_patterns = [
            r'Exception in thread',
            r'java\.lang\.\w*Exception',
            r'Error: (.+)',
            r'FATAL: (.+)',
            r'Failed to (.+)',
            r'Connection refused',
            r'Timeout',
            r'Out of memory'
        ]
        
        self.warning_patterns = [
            r'Warning: (.+)',
            r'WARN: (.+)',
            r'Deprecated',
            r'Retry attempt'
        ]
    
    def analyze_log_file(self, log_file):
        """åˆ†ææ—¥å¿—æ–‡ä»¶ï¼Œæå–å¼‚å¸¸æ¨¡å¼"""
        
        analysis = {
            'summary': {
                'total_lines': 0,
                'error_count': 0,
                'warning_count': 0,
                'unique_errors': set(),
                'error_timeline': defaultdict(int)
            },
            'errors': [],
            'warnings': [],
            'patterns': {
                'frequent_errors': Counter(),
                'error_sequences': [],
                'performance_issues': []
            }
        }
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        for i, line in enumerate(lines):
            analysis['summary']['total_lines'] += 1
            timestamp = self._extract_timestamp(line)
            
            # æ£€æµ‹é”™è¯¯
            for pattern in self.error_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    error_info = {
                        'line_number': i + 1,
                        'timestamp': timestamp,
                        'pattern': pattern,
                        'message': line.strip(),
                        'context': self._get_context(lines, i)
                    }
                    
                    analysis['errors'].append(error_info)
                    analysis['summary']['error_count'] += 1
                    analysis['summary']['unique_errors'].add(match.group(0))
                    analysis['patterns']['frequent_errors'][match.group(0)] += 1
                    
                    if timestamp:
                        hour_key = timestamp.strftime('%Y-%m-%d %H:00')
                        analysis['summary']['error_timeline'][hour_key] += 1
                    break
            
            # æ£€æµ‹è­¦å‘Š
            for pattern in self.warning_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    warning_info = {
                        'line_number': i + 1,
                        'timestamp': timestamp,
                        'message': line.strip()
                    }
                    analysis['warnings'].append(warning_info)
                    analysis['summary']['warning_count'] += 1
                    break
            
            # æ£€æµ‹æ€§èƒ½é—®é¢˜
            if 'took' in line.lower() or 'duration' in line.lower():
                duration_match = re.search(r'(\d+(?:\.\d+)?)\s*(ms|seconds?|minutes?)', line, re.IGNORECASE)
                if duration_match:
                    duration = float(duration_match.group(1))
                    unit = duration_match.group(2).lower()
                    
                    # è½¬æ¢ä¸ºæ¯«ç§’
                    if 'second' in unit:
                        duration *= 1000
                    elif 'minute' in unit:
                        duration *= 60000
                    
                    if duration > 5000:  # è¶…è¿‡5ç§’è®¤ä¸ºæ˜¯æ€§èƒ½é—®é¢˜
                        analysis['patterns']['performance_issues'].append({
                            'line_number': i + 1,
                            'duration': duration,
                            'message': line.strip()
                        })
        
        # æ£€æµ‹é”™è¯¯åºåˆ—
        analysis['patterns']['error_sequences'] = self._detect_error_sequences(analysis['errors'])
        
        return analysis
    
    def _extract_timestamp(self, line):
        """ä»æ—¥å¿—è¡Œä¸­æå–æ—¶é—´æˆ³"""
        patterns = [
            r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
            r'\w{3} \d{2} \d{2}:\d{2}:\d{2}'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                try:
                    return datetime.strptime(match.group(0), '%Y-%m-%d %H:%M:%S')
                except:
                    continue
        return None
    
    def _get_context(self, lines, index, context_size=3):
        """è·å–é”™è¯¯è¡Œçš„ä¸Šä¸‹æ–‡"""
        start = max(0, index - context_size)
        end = min(len(lines), index + context_size + 1)
        
        context = []
        for i in range(start, end):
            prefix = ">>> " if i == index else "    "
            context.append(f"{prefix}{i+1}: {lines[i].rstrip()}")
        
        return "\n".join(context)
    
    def _detect_error_sequences(self, errors):
        """æ£€æµ‹é”™è¯¯åºåˆ—æ¨¡å¼"""
        sequences = []
        
        if len(errors) < 2:
            return sequences
        
        for i in range(len(errors) - 1):
            current_error = errors[i]
            next_error = errors[i + 1]
            
            if (current_error['timestamp'] and next_error['timestamp'] and
                (next_error['timestamp'] - current_error['timestamp']).seconds < 60):
                
                sequences.append({
                    'start_line': current_error['line_number'],
                    'end_line': next_error['line_number'],
                    'duration': (next_error['timestamp'] - current_error['timestamp']).seconds,
                    'error_count': 2
                })
        
        return sequences
    
    def generate_report(self, analysis):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = f"""
# æ—¥å¿—åˆ†ææŠ¥å‘Š

## æ¦‚è§ˆ
- æ€»è¡Œæ•°: {analysis['summary']['total_lines']:,}
- é”™è¯¯æ•°: {analysis['summary']['error_count']:,}
- è­¦å‘Šæ•°: {analysis['summary']['warning_count']:,}
- å”¯ä¸€é”™è¯¯ç±»å‹: {len(analysis['summary']['unique_errors'])}

## é«˜é¢‘é”™è¯¯
"""
        
        for error, count in analysis['patterns']['frequent_errors'].most_common(10):
            report += f"- {error}: {count}æ¬¡\n"
        
        if analysis['patterns']['performance_issues']:
            report += f"\n## æ€§èƒ½é—®é¢˜ ({len(analysis['patterns']['performance_issues'])}ä¸ª)\n"
            for issue in analysis['patterns']['performance_issues'][:5]:
                report += f"- è¡Œ{issue['line_number']}: {issue['duration']:.0f}ms\n"
        
        if analysis['patterns']['error_sequences']:
            report += f"\n## é”™è¯¯åºåˆ— ({len(analysis['patterns']['error_sequences'])}ä¸ª)\n"
            for seq in analysis['patterns']['error_sequences'][:5]:
                report += f"- è¡Œ{seq['start_line']}-{seq['end_line']}: {seq['duration']}ç§’å†…è¿ç»­é”™è¯¯\n"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = LogAnalyzer()
analysis = analyzer.analyze_log_file('build.log')
report = analyzer.generate_report(analysis)
print(report)
```

## æ•…éšœæ’æŸ¥

### 1. å¸¸è§é—®é¢˜è¯Šæ–­

#### æ„å»ºå¤±è´¥è¯Šæ–­
```bash
#!/bin/bash

# æ„å»ºå¤±è´¥è¯Šæ–­è„šæœ¬
diagnose_build_failure() {
    local build_log=$1
    local project_id=$2
    local pipeline_id=$3
    local build_id=$4
    
    echo "=== æ„å»ºå¤±è´¥è¯Šæ–­æŠ¥å‘Š ==="
    echo "é¡¹ç›®ID: $project_id"
    echo "æµæ°´çº¿ID: $pipeline_id"
    echo "æ„å»ºID: $build_id"
    echo "æ—¥å¿—æ–‡ä»¶: $build_log"
    echo ""
    
    # æ£€æŸ¥å¸¸è§é”™è¯¯æ¨¡å¼
    echo "=== é”™è¯¯æ¨¡å¼åˆ†æ ==="
    
    # ç¼–è¯‘é”™è¯¯
    if grep -q "compilation failed\|compile error\|build failed" "$build_log"; then
        echo "âŒ å‘ç°ç¼–è¯‘é”™è¯¯"
        grep -n -A 3 -B 1 "compilation failed\|compile error\|build failed" "$build_log" | head -20
        echo ""
    fi
    
    # æµ‹è¯•å¤±è´¥
    if grep -q "test failed\|tests failed\|assertion failed" "$build_log"; then
        echo "âŒ å‘ç°æµ‹è¯•å¤±è´¥"
        grep -n -A 3 -B 1 "test failed\|tests failed\|assertion failed" "$build_log" | head -20
        echo ""
    fi
    
    # ç½‘ç»œé—®é¢˜
    if grep -q "connection refused\|timeout\|network error\|dns resolution failed" "$build_log"; then
        echo "ğŸŒ å‘ç°ç½‘ç»œé—®é¢˜"
        grep -n -A 2 -B 1 "connection refused\|timeout\|network error\|dns resolution failed" "$build_log" | head -10
        echo ""
    fi
    
    # æƒé™é—®é¢˜
    if grep -q "permission denied\|access denied\|unauthorized" "$build_log"; then
        echo "ğŸ”’ å‘ç°æƒé™é—®é¢˜"
        grep -n -A 2 -B 1 "permission denied\|access denied\|unauthorized" "$build_log" | head -10
        echo ""
    fi
    
    # èµ„æºä¸è¶³
    if grep -q "out of memory\|disk space\|no space left" "$build_log"; then
        echo "ğŸ’¾ å‘ç°èµ„æºä¸è¶³é—®é¢˜"
        grep -n -A 2 -B 1 "out of memory\|disk space\|no space left" "$build_log" | head -10
        echo ""
    fi
    
    # ä¾èµ–é—®é¢˜
    if grep -q "dependency not found\|module not found\|package not found" "$build_log"; then
        echo "ğŸ“¦ å‘ç°ä¾èµ–é—®é¢˜"
        grep -n -A 2 -B 1 "dependency not found\|module not found\|package not found" "$build_log" | head -10
        echo ""
    fi
    
    # ç»Ÿè®¡ä¿¡æ¯
    echo "=== ç»Ÿè®¡ä¿¡æ¯ ==="
    echo "æ€»è¡Œæ•°: $(wc -l < "$build_log")"
    echo "é”™è¯¯è¡Œæ•°: $(grep -c -i "error\|failed\|exception" "$build_log")"
    echo "è­¦å‘Šè¡Œæ•°: $(grep -c -i "warning\|warn" "$build_log")"
    echo ""
    
    # å»ºè®®
    echo "=== æ’æŸ¥å»ºè®® ==="
    if grep -q "compilation failed" "$build_log"; then
        echo "1. æ£€æŸ¥ä»£ç è¯­æ³•é”™è¯¯"
        echo "2. ç¡®è®¤ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§"
        echo "3. æ£€æŸ¥ç¼–è¯‘ç¯å¢ƒé…ç½®"
    fi
    
    if grep -q "test failed" "$build_log"; then
        echo "1. è¿è¡Œæœ¬åœ°æµ‹è¯•ç¡®è®¤é—®é¢˜"
        echo "2. æ£€æŸ¥æµ‹è¯•æ•°æ®å’Œç¯å¢ƒ"
        echo "3. ç¡®è®¤æµ‹è¯•ç”¨ä¾‹çš„æ­£ç¡®æ€§"
    fi
    
    if grep -q "timeout\|network error" "$build_log"; then
        echo "1. æ£€æŸ¥ç½‘ç»œè¿æ¥"
        echo "2. ç¡®è®¤é˜²ç«å¢™è®¾ç½®"
        echo "3. è€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´"
    fi
}

# ä½¿ç”¨ç¤ºä¾‹
diagnose_build_failure "build.log" "project-123" "pipeline-456" "build-789"
```

### 2. æ€§èƒ½é—®é¢˜æ’æŸ¥

#### æ„å»ºæ€§èƒ½åˆ†æ
```python
#!/usr/bin/env python3
import re
import json
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd

class BuildPerformanceDiagnostic:
    def __init__(self, log_file):
        self.log_file = log_file
        self.timeline = []
        self.plugins = {}
        self.bottlenecks = []
    
    def analyze_performance(self):
        """åˆ†ææ„å»ºæ€§èƒ½"""
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_plugin = None
        plugin_start_time = None
        
        for line in lines:
            timestamp = self._extract_timestamp(line)
            if not timestamp:
                continue
            
            # æ£€æµ‹æ’ä»¶å¼€å§‹
            plugin_start_match = re.search(r'Starting plugin: (\w+)', line)
            if plugin_start_match:
                current_plugin = plugin_start_match.group(1)
                plugin_start_time = timestamp
                continue
            
            # æ£€æµ‹æ’ä»¶ç»“æŸ
            plugin_end_match = re.search(r'Plugin (\w+) completed in (\d+)ms', line)
            if plugin_end_match:
                plugin_name = plugin_end_match.group(1)
                duration = int(plugin_end_match.group(2))
                
                if plugin_name not in self.plugins:
                    self.plugins[plugin_name] = []
                
                self.plugins[plugin_name].append({
                    'duration': duration,
                    'timestamp': timestamp
                })
                
                # è®°å½•åˆ°æ—¶é—´çº¿
                self.timeline.append({
                    'timestamp': timestamp,
                    'plugin': plugin_name,
                    'duration': duration,
                    'type': 'plugin_completion'
                })
                
                # æ£€æµ‹æ€§èƒ½ç“¶é¢ˆ
                if duration > 60000:  # è¶…è¿‡1åˆ†é’Ÿ
                    self.bottlenecks.append({
                        'plugin': plugin_name,
                        'duration': duration,
                        'timestamp': timestamp,
                        'severity': 'high' if duration > 300000 else 'medium'
                    })
        
        return self._generate_performance_report()
    
    def _extract_timestamp(self, line):
        """æå–æ—¶é—´æˆ³"""
        match = re.search(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', line)
        if match:
            try:
                return datetime.strptime(match.group(0), '%Y-%m-%d %H:%M:%S')
            except:
                pass
        return None
    
    def _generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        report = {
            'summary': {
                'total_plugins': len(self.plugins),
                'total_bottlenecks': len(self.bottlenecks),
                'slowest_plugins': [],
                'performance_score': 0
            },
            'plugin_analysis': {},
            'bottlenecks': self.bottlenecks,
            'recommendations': []
        }
        
        # åˆ†ææ¯ä¸ªæ’ä»¶
        plugin_stats = []
        for plugin_name, executions in self.plugins.items():
            durations = [exec['duration'] for exec in executions]
            
            stats = {
                'name': plugin_name,
                'executions': len(executions),
                'avg_duration': sum(durations) / len(durations),
                'max_duration': max(durations),
                'min_duration': min(durations),
                'total_time': sum(durations)
            }
            
            plugin_stats.append(stats)
            report['plugin_analysis'][plugin_name] = stats
        
        # æ’åºæ‰¾å‡ºæœ€æ…¢çš„æ’ä»¶
        plugin_stats.sort(key=lambda x: x['avg_duration'], reverse=True)
        report['summary']['slowest_plugins'] = plugin_stats[:5]
        
        # è®¡ç®—æ€§èƒ½è¯„åˆ†
        total_time = sum(stats['total_time'] for stats in plugin_stats)
        if total_time > 0:
            bottleneck_time = sum(b['duration'] for b in self.bottlenecks)
            report['summary']['performance_score'] = max(0, 100 - (bottleneck_time / total_time * 100))
        
        # ç”Ÿæˆå»ºè®®
        report['recommendations'] = self._generate_recommendations(plugin_stats)
        
        return report
    
    def _generate_recommendations(self, plugin_stats):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ£€æŸ¥æ…¢æ’ä»¶
        for stats in plugin_stats[:3]:
            if stats['avg_duration'] > 120000:  # è¶…è¿‡2åˆ†é’Ÿ
                recommendations.append({
                    'type': 'slow_plugin',
                    'plugin': stats['name'],
                    'message': f"æ’ä»¶ {stats['name']} å¹³å‡æ‰§è¡Œæ—¶é—´ {stats['avg_duration']/1000:.1f}ç§’ï¼Œå»ºè®®ä¼˜åŒ–",
                    'suggestions': [
                        "æ£€æŸ¥æ’ä»¶é…ç½®æ˜¯å¦åˆç†",
                        "è€ƒè™‘å¹¶è¡Œæ‰§è¡Œæˆ–ç¼“å­˜ä¼˜åŒ–",
                        "æ£€æŸ¥ç½‘ç»œä¾èµ–å’Œèµ„æºè®¿é—®"
                    ]
                })
        
        # æ£€æŸ¥é‡å¤æ‰§è¡Œ
        for stats in plugin_stats:
            if stats['executions'] > 3:
                recommendations.append({
                    'type': 'repeated_execution',
                    'plugin': stats['name'],
                    'message': f"æ’ä»¶ {stats['name']} æ‰§è¡Œäº† {stats['executions']} æ¬¡ï¼Œå¯èƒ½å­˜åœ¨é‡å¤",
                    'suggestions': [
                        "æ£€æŸ¥æµæ°´çº¿é…ç½®æ˜¯å¦æœ‰é‡å¤æ­¥éª¤",
                        "è€ƒè™‘åˆå¹¶ç›¸ä¼¼çš„æ’ä»¶æ‰§è¡Œ",
                        "ä½¿ç”¨æ¡ä»¶æ‰§è¡Œé¿å…ä¸å¿…è¦çš„é‡å¤"
                    ]
                })
        
        return recommendations
    
    def visualize_performance(self, output_file='performance_analysis.png'):
        """å¯è§†åŒ–æ€§èƒ½åˆ†æç»“æœ"""
        if not self.plugins:
            return
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # æ’ä»¶æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
        plugin_names = list(self.plugins.keys())
        avg_durations = []
        
        for plugin in plugin_names:
            durations = [exec['duration'] for exec in self.plugins[plugin]]
            avg_durations.append(sum(durations) / len(durations) / 1000)  # è½¬æ¢ä¸ºç§’
        
        ax1.bar(plugin_names, avg_durations)
        ax1.set_title('æ’ä»¶å¹³å‡æ‰§è¡Œæ—¶é—´')
        ax1.set_ylabel('æ—¶é—´ (ç§’)')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ€§èƒ½ç“¶é¢ˆæ—¶é—´çº¿
        if self.bottlenecks:
            bottleneck_times = [b['timestamp'] for b in self.bottlenecks]
            bottleneck_durations = [b['duration'] / 1000 for b in self.bottlenecks]
            
            ax2.scatter(bottleneck_times, bottleneck_durations, c='red', alpha=0.7)
            ax2.set_title('æ€§èƒ½ç“¶é¢ˆæ—¶é—´çº¿')
            ax2.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
            ax2.set_xlabel('æ—¶é—´')
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_file

# ä½¿ç”¨ç¤ºä¾‹
diagnostic = BuildPerformanceDiagnostic('build.log')
report = diagnostic.analyze_performance()

print("=== æ„å»ºæ€§èƒ½åˆ†ææŠ¥å‘Š ===")
print(f"æ€»æ’ä»¶æ•°: {report['summary']['total_plugins']}")
print(f"æ€§èƒ½ç“¶é¢ˆæ•°: {report['summary']['total_bottlenecks']}")
print(f"æ€§èƒ½è¯„åˆ†: {report['summary']['performance_score']:.1f}/100")

print("\n=== æœ€æ…¢çš„æ’ä»¶ ===")
for plugin in report['summary']['slowest_plugins']:
    print(f"{plugin['name']}: {plugin['avg_duration']/1000:.1f}ç§’")

print("\n=== ä¼˜åŒ–å»ºè®® ===")
for rec in report['recommendations']:
    print(f"- {rec['message']}")
```

## æœ€ä½³å®è·µ

### 1. ç›‘æ§ç­–ç•¥è®¾è®¡

#### åˆ†å±‚ç›‘æ§ä½“ç³»
```yaml
monitoring_strategy:
  # åŸºç¡€è®¾æ–½å±‚
  infrastructure:
    metrics:
      - cpu_usage
      - memory_usage
      - disk_usage
      - network_io
    alerts:
      - threshold: 80%
        severity: warning
      - threshold: 95%
        severity: critical
  
  # åº”ç”¨å±‚
  application:
    metrics:
      - pipeline_success_rate
      - build_duration
      - queue_time
      - plugin_performance
    alerts:
      - success_rate < 90%
      - build_duration > 30min
      - queue_time > 10min
  
  # ä¸šåŠ¡å±‚
  business:
    metrics:
      - deployment_frequency
      - lead_time
      - mttr
      - change_failure_rate
    alerts:
      - deployment_frequency < daily
      - lead_time > 1day
      - change_failure_rate > 15%
```

### 2. æ—¥å¿—ç®¡ç†è§„èŒƒ

#### æ—¥å¿—ä¿ç•™ç­–ç•¥
```yaml
log_retention_policy:
  # æ„å»ºæ—¥å¿—
  build_logs:
    retention: "60 days"
    compression: true
    archive_location: "s3://logs-archive/builds/"
  
  # ç³»ç»Ÿæ—¥å¿—
  system_logs:
    retention: "30 days"
    rotation: "daily"
    max_size: "100MB"
  
  # å®¡è®¡æ—¥å¿—
  audit_logs:
    retention: "1 year"
    encryption: true
    immutable: true
  
  # é”™è¯¯æ—¥å¿—
  error_logs:
    retention: "90 days"
    priority: "high"
    alerting: true
```

### 3. å‘Šè­¦ç®¡ç†

#### å‘Šè­¦åˆ†çº§å’Œå¤„ç†
```yaml
alert_management:
  # å‘Šè­¦çº§åˆ«
  levels:
    critical:
      response_time: "5 minutes"
      escalation: "immediate"
      channels: ["phone", "sms", "email"]
    
    warning:
      response_time: "30 minutes"
      escalation: "1 hour"
      channels: ["email", "wework"]
    
    info:
      response_time: "2 hours"
      escalation: "none"
      channels: ["email"]
  
  # å‘Šè­¦æŠ‘åˆ¶
  suppression:
    - name: "maintenance_window"
      schedule: "0 2 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
      duration: "4 hours"
    
    - name: "known_issues"
      conditions:
        - alert_name: "disk_space_low"
          duration: "1 hour"
  
  # å‘Šè­¦èšåˆ
  aggregation:
    - name: "build_failures"
      group_by: ["pipeline_id"]
      group_wait: "30s"
      group_interval: "5m"
      repeat_interval: "1h"
```

## æ€»ç»“

ç›‘æ§å’Œæ—¥å¿—ç®¡ç†æ˜¯ä¿éšœCI/CDç³»ç»Ÿç¨³å®šè¿è¡Œçš„é‡è¦æ‰‹æ®µï¼š

1. **å…¨é¢ç›‘æ§**: ä»åŸºç¡€è®¾æ–½åˆ°ä¸šåŠ¡æŒ‡æ ‡çš„å¤šå±‚æ¬¡ç›‘æ§
2. **æ™ºèƒ½å‘Šè­¦**: åŸºäºé˜ˆå€¼å’Œè¶‹åŠ¿çš„æ™ºèƒ½å‘Šè­¦æœºåˆ¶
3. **é«˜æ•ˆæ—¥å¿—**: ç»“æ„åŒ–æ—¥å¿—å’Œæ™ºèƒ½åˆ†æå·¥å…·
4. **å¿«é€Ÿæ’æŸ¥**: å®Œå–„çš„æ•…éšœè¯Šæ–­å’Œæ€§èƒ½åˆ†æå·¥å…·

å»ºè®®æ ¹æ®å›¢é˜Ÿè§„æ¨¡å’Œä¸šåŠ¡éœ€æ±‚ï¼Œå»ºç«‹é€‚åˆçš„ç›‘æ§å’Œæ—¥å¿—ç®¡ç†ä½“ç³»ï¼Œå¹¶æŒç»­ä¼˜åŒ–ç›‘æ§ç­–ç•¥å’Œå‘Šè­¦è§„åˆ™ã€‚